{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Tokens"
      ],
      "metadata": {
        "id": "L6R6w86v_GXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tiktoken import encoding_for_model\n",
        "\n",
        "# Initialize encoder for a given model\n",
        "encoder = encoding_for_model(\"text-davinci-003\")\n",
        "\n",
        "# Text to encode\n",
        "text = \"The sun is shining\"\n",
        "\n",
        "# Encoding the text\n",
        "tokens = encoder.encode(text)\n",
        "\n",
        "# Output the encoded tokens\n",
        "print(tokens)  # This will output the tokens as integers\n",
        "\n",
        "# Decoding each token back to text\n",
        "decoded_tokens = [encoder.decode_single_token_bytes(token).decode('utf-8') for token in tokens]\n",
        "\n",
        "# Output the decoded tokens\n",
        "print(decoded_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9Hnnmg9_LiR",
        "outputId": "333ef8bf-c3c6-4fe9-93b1-f2732c16bf48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[464, 4252, 318, 22751]\n",
            "['The', ' sun', ' is', ' shining']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The OpenAI API"
      ],
      "metadata": {
        "id": "DwzCUQVu8nB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Replace 'YourKey' with your actual OpenAI API key\n",
        "client = OpenAI(api_key='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')\n",
        "\n",
        "# Specify the model you want to use\n",
        "model = \"gpt-3.5-turbo\"  # or \"gpt-4-1106-preview\" for GPT-4 if available\n",
        "\n",
        "# System and user messages\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": 'You are a Tech Support Bot that provides helpful and concise tech advice.'},\n",
        "    {\"role\": \"user\", \"content\": 'How do I reset my router?'}\n",
        "]\n",
        "\n",
        "# Creating a chat completion\n",
        "response = client.chat.completions.create(\n",
        "    model=model,\n",
        "    messages=messages,\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Extracting and printing the response message\n",
        "response_message = response.choices[0].message.content\n",
        "print(response_message)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEyBPCAH8kAB",
        "outputId": "9ccd91ef-319b-421e-fc12-4b3c384688b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To reset your router, locate the reset button on the back or bottom of the router. Use a paperclip or a small pointed object to press and hold the reset button for about 10-15 seconds until the router lights start flashing. This will reset the router to its factory default settings. Remember to reconfigure your network settings after the reset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create our own code interpreter"
      ],
      "metadata": {
        "id": "u3FFZFrLCa4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import json\n",
        "import inspect\n",
        "from pydantic import create_model\n",
        "from inspect import Parameter"
      ],
      "metadata": {
        "id": "haAc2snNChe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate factorial\n",
        "def factorial(n: int):\n",
        "    \"\"\"Calculates factorial of n\"\"\"\n",
        "    if n == 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return n * factorial(n-1)"
      ],
      "metadata": {
        "id": "aVV7eigrC9U4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate arithmetic expressions\n",
        "def eval_expr(expr: str):\n",
        "    \"\"\"Evaluates a basic arithmetic expression\"\"\"\n",
        "    try:\n",
        "        # Safely evaluate arithmetic expressions\n",
        "        tree = ast.parse(expr, mode='eval')\n",
        "        if isinstance(tree, ast.Expression):\n",
        "            compiled = compile(tree, filename=\"<ast>\", mode=\"eval\")\n",
        "            return eval(compiled)\n",
        "        else:\n",
        "            return \"Invalid expression\"\n",
        "    except:\n",
        "        return \"Error in expression\""
      ],
      "metadata": {
        "id": "XzvSWKrwC_jR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate schema for functions\n",
        "def schema(f):\n",
        "    kw = {n: (o.annotation, ... if o.default == Parameter.empty else o.default)\n",
        "          for n, o in inspect.signature(f).parameters.items()}\n",
        "    s = create_model(f'Input for `{f.__name__}`', **kw).schema()\n",
        "    return dict(name=f.__name__, description=f.__doc__, parameters=s)"
      ],
      "metadata": {
        "id": "OEN5b-KDDBkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the setup for GPT model interaction\n",
        "def askgpt(question, system=None, functions=[]):\n",
        "    # Simulate the call to GPT model with the provided question and system message\n",
        "    # This is a placeholder for actual GPT model interaction\n",
        "    print(f\"Question: {question}\")\n",
        "    if system:\n",
        "        print(f\"System: {system}\")\n",
        "    for func in functions:\n",
        "        print(f\"Function schema: {json.dumps(func, indent=2)}\")\n",
        "    # Placeholder response simulation\n",
        "    return {\"choices\": [{\"message\": {\"function_call\": {\"name\": \"factorial\", \"arguments\": json.dumps({\"n\": 5})}}}]}"
      ],
      "metadata": {
        "id": "D-2cpWJMDDZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example use of the schema function\n",
        "print(\"Factorial function schema:\")\n",
        "print(schema(factorial))\n",
        "\n",
        "# Example use of the eval_expr function\n",
        "print(\"\\nEvaluating expression '2 + 3 * 4':\")\n",
        "print(eval_expr(\"2 + 3 * 4\"))"
      ],
      "metadata": {
        "id": "cl7BJoWPDGqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example interaction with GPT model\n",
        "c = askgpt(\"Calculate 5 factorial using the `factorial` function.\",\n",
        "           system=\"You must use the `factorial` function for calculations.\",\n",
        "           functions=[schema(factorial)])\n",
        "\n",
        "# Function to call the appropriate Python function based on GPT's response\n",
        "def call_func(c):\n",
        "    fc = c[\"choices\"][0][\"message\"][\"function_call\"]\n",
        "    f = globals()[fc[\"name\"]]\n",
        "    args = json.loads(fc[\"arguments\"])\n",
        "    return f(**args)\n",
        "\n",
        "# Example function call based on GPT's response\n",
        "print(\"\\nCalling function based on GPT's response:\")\n",
        "print(call_func(c))"
      ],
      "metadata": {
        "id": "rt32o01YDJhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval augmented generation(RAG)"
      ],
      "metadata": {
        "id": "thkluMzcEYsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbfzv4_190mX",
        "outputId": "2da4ccc9-d3e1-489b-f8b2-8e80bb219d26"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "H1xGvQ1094JF",
        "outputId": "24667fe5-ebfe-4cde-9aae-512d93ab6ee6"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "str expected, not NoneType",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-2470e233c07c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OPENAI_API_KEY'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/os.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencodekey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0mputenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/os.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"str expected, not %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'surrogateescape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: str expected, not NoneType"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3YqonQZO_pag"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}